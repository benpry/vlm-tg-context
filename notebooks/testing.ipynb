{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7101103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 13:51:10 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pyprojroot import here\n",
    "from src.utils import preprocess_messages\n",
    "from src.lm import SYSTEM_PROMPT\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=\"/scr/benpry/conda/envs/vtc/lib64:/scr/benpry/conda/envs/vtc/lib:$LD_LIBRARY_PATH\"\n",
    "!export CUDA_HOME=\"/usr/local/cuda-12.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dad093",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "IMAGE = Image.open(here(\"data/compiled_grid.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(here(\"context_prep/yoked.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3372679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "/sailhome/benpry/vlm-tg-context/src/utils.py:33: UserWarning: Message {'role': 'describer', 'message_number': 2} is missing 'text' field.\n",
      "  warnings.warn(f\"Message {message} is missing 'text' field.\")\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL)\n",
    "df[\"chat_prompt\"] = df.apply(preprocess_messages, axis=1)\n",
    "df[\"chat_messages\"] = df[\"chat_prompt\"].apply(\n",
    "    lambda x: [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "                {\"type\": \"image\", \"image\": IMAGE},\n",
    "            ],\n",
    "        },\n",
    "        *x,\n",
    "    ]\n",
    ")\n",
    "df[\"formatted_prompt\"] = df[\"chat_messages\"].apply(\n",
    "    lambda x: processor.apply_chat_template(\n",
    "        x, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3857f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'You will be presented with a list of messages between people playing a reference game, where the describer has to get the matcher to choose an image from a list of images. Your goal is to guess which of the images the describer is trying to get the matcher to choose. The images, with their labels, are shown in the image.\\nPlease answer with just the letter corresponding to the image you think the describer is trying to get the matcher to choose.\\n'}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=860x790 at 0x7FB7AC144BC0>}]}, {'role': 'user', 'content': 'describer: triangle at the bottom, right angled triangle above, 2 rotated squares on top of that They are touching on their points\\n'}, {'role': 'assistant', 'content': 'K'}, {'role': 'user', 'content': 'describer: This one looks like a person sat down Their head is a rotated square\\nmatcher: do they have bunny ears\\nmatcher: or knee up\\nmatcher: sat down with legs out\\ndescriber: They look as if they are sad so the head is tilted down\\ndescriber: 1 knee up no bunny ears\\n'}]\n",
      "<|im_start|>system\n",
      "You will be presented with a list of messages between people playing a reference game, where the describer has to get the matcher to choose an image from a list of images. Your goal is to guess which of the images the describer is trying to get the matcher to choose. The images, with their labels, are shown in the image.\n",
      "Please answer with just the letter corresponding to the image you think the describer is trying to get the matcher to choose.\n",
      "<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>user\n",
      "describer: triangle at the bottom, right angled triangle above, 2 rotated squares on top of that They are touching on their points\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "K<|im_end|>\n",
      "<|im_start|>user\n",
      "describer: This one looks like a person sat down Their head is a rotated square\n",
      "matcher: do they have bunny ears\n",
      "matcher: or knee up\n",
      "matcher: sat down with legs out\n",
      "describer: They look as if they are sad so the head is tilted down\n",
      "describer: 1 knee up no bunny ears\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[1]\n",
    "print(row[\"chat_messages\"])\n",
    "print(row[\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169e2511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 13:51:36 [config.py:1604] Using max model len 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 13:51:38,191\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 13:51:38 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-08 13:51:38 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 08-08 13:51:42 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-08 13:51:43 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-08 13:51:43 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-VL-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-VL-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":16,\"local_cache_dir\":null}\n",
      "WARNING 08-08 13:51:43 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-08 13:51:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_f04cec1d'), local_subscribe_addr='ipc:///tmp/user/22065/1b0e2f36-3e1b-4284-9b3b-4009b7f638df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 08-08 13:51:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-08 13:51:47 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_23f506ff'), local_subscribe_addr='ipc:///tmp/user/22065/d5b8a1f4-d7f0-45ec-be13-fd7675fa9cd2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_782152fb'), local_subscribe_addr='ipc:///tmp/user/22065/9e1848c5-da6c-4e9d-b7d0-afdf5eab8139', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:50 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:50 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:51 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 08-08 13:51:51 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_69e56dcd'), local_subscribe_addr='ipc:///tmp/user/22065/fd4ab938-1135-43a2-9e28-4f2b5a23ef09', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:51 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:51 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m WARNING 08-08 13:51:52 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:52 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:52 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen2.5-VL-7B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m WARNING 08-08 13:51:52 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:52 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:52 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen2.5-VL-7B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m WARNING 08-08 13:51:52 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:52 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m WARNING 08-08 13:51:52 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:52 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:52 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.78it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  2.97it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  2.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  2.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  2.56it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:54 [default_loader.py:262] Loading weights took 2.00 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:55 [gpu_model_runner.py:1892] Model loading took 7.8681 GiB and 2.584217 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:55 [default_loader.py:262] Loading weights took 2.39 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:55 [gpu_model_runner.py:1892] Model loading took 7.8681 GiB and 3.112804 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:51:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:51:56 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:52:08 [backends.py:530] Using cache directory: /afs/cs.stanford.edu/u/benpry/.cache/vllm/torch_compile_cache/01442316c6/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:52:08 [backends.py:541] Dynamo bytecode transform time: 6.11 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:52:09 [backends.py:530] Using cache directory: /afs/cs.stanford.edu/u/benpry/.cache/vllm/torch_compile_cache/01442316c6/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:52:09 [backends.py:541] Dynamo bytecode transform time: 7.10 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:52:13 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.016 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:52:15 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.983 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m INFO 08-08 13:52:17 [monitor.py:34] torch.compile takes 7.10 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m INFO 08-08 13:52:17 [monitor.py:34] torch.compile takes 6.11 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 199, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     subprocess.run(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/subprocess.py\", line 571, in run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/sailhome/benpry/.cache/flashinfer/80/cached_ops', '-f', '/sailhome/benpry/.cache/flashinfer/80/cached_ops/sampling/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 541, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 233, in determine_available_memory\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2421, in profile_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2246, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise e\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2236, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 107, in forward_cuda\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return flashinfer_sample(logits.contiguous(), k, p, generators)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 293, in flashinfer_sample\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     next_token_ids = flashinfer.sampling.top_k_top_p_sampling_from_logits(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 983, in top_k_top_p_sampling_from_logits\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     masked_logits = top_k_mask_logits(logits, top_k)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 1303, in top_k_mask_logits\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return get_sampling_module().top_k_mask_logits(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 47, in get_sampling_module\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     module = gen_sampling_module().build_and_load()\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 123, in build_and_load\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.build(verbose)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 115, in build\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 211, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise RuntimeError(msg) from e\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] [1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] collect2: error: ld returned 1 exit status\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: build stopped: subcommand failed.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 199, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     subprocess.run(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/subprocess.py\", line 571, in run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/sailhome/benpry/.cache/flashinfer/80/cached_ops', '-f', '/sailhome/benpry/.cache/flashinfer/80/cached_ops/sampling/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 541, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 233, in determine_available_memory\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2421, in profile_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2246, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise e\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2236, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 107, in forward_cuda\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return flashinfer_sample(logits.contiguous(), k, p, generators)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 293, in flashinfer_sample\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     next_token_ids = flashinfer.sampling.top_k_top_p_sampling_from_logits(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 983, in top_k_top_p_sampling_from_logits\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     masked_logits = top_k_mask_logits(logits, top_k)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 1303, in top_k_mask_logits\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return get_sampling_module().top_k_mask_logits(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 47, in get_sampling_module\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     module = gen_sampling_module().build_and_load()\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 123, in build_and_load\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.build(verbose)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 115, in build\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 211, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise RuntimeError(msg) from e\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] [1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] collect2: error: ld returned 1 exit status\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: build stopped: subcommand failed.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m /scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1248267)\u001b[0;0m   warnings.warn(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m /scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 199, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     subprocess.run(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/subprocess.py\", line 571, in run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/sailhome/benpry/.cache/flashinfer/80/cached_ops', '-f', '/sailhome/benpry/.cache/flashinfer/80/cached_ops/sampling/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 541, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 233, in determine_available_memory\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2421, in profile_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2246, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise e\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2236, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 107, in forward_cuda\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return flashinfer_sample(logits.contiguous(), k, p, generators)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 293, in flashinfer_sample\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     next_token_ids = flashinfer.sampling.top_k_top_p_sampling_from_logits(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 983, in top_k_top_p_sampling_from_logits\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     masked_logits = top_k_mask_logits(logits, top_k)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 1303, in top_k_mask_logits\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return get_sampling_module().top_k_mask_logits(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 47, in get_sampling_module\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     module = gen_sampling_module().build_and_load()\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 123, in build_and_load\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.build(verbose)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 115, in build\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 211, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise RuntimeError(msg) from e\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] [1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] collect2: error: ld returned 1 exit status\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: build stopped: subcommand failed.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 199, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     subprocess.run(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/subprocess.py\", line 571, in run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise CalledProcessError(retcode, process.args,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] subprocess.CalledProcessError: Command '['ninja', '-v', '-C', '/sailhome/benpry/.cache/flashinfer/80/cached_ops', '-f', '/sailhome/benpry/.cache/flashinfer/80/cached_ops/sampling/build.ninja']' returned non-zero exit status 1.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 541, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 233, in determine_available_memory\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2421, in profile_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2246, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise e\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2236, in _dummy_sampler_run\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 107, in forward_cuda\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return flashinfer_sample(logits.contiguous(), k, p, generators)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 293, in flashinfer_sample\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     next_token_ids = flashinfer.sampling.top_k_top_p_sampling_from_logits(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 983, in top_k_top_p_sampling_from_logits\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     masked_logits = top_k_mask_logits(logits, top_k)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 1303, in top_k_mask_logits\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     return get_sampling_module().top_k_mask_logits(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/sampling.py\", line 47, in get_sampling_module\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     module = gen_sampling_module().build_and_load()\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 123, in build_and_load\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     self.build(verbose)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/core.py\", line 115, in build\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     run_ninja(jit_env.FLASHINFER_JIT_DIR, self.ninja_path, verbose)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/flashinfer/jit/cpp_ext.py\", line 211, in run_ninja\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546]     raise RuntimeError(msg) from e\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] RuntimeError: Ninja build failed. Ninja output:\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] [1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] /scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] collect2: error: ld returned 1 exit status\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] ninja: build stopped: subcommand failed.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1248266)\u001b[0;0m ERROR 08-08 13:52:17 [multiproc_executor.py:546] \n",
      "ERROR 08-08 13:52:17 [core.py:632] EngineCore failed to start.\n",
      "ERROR 08-08 13:52:17 [core.py:632] Traceback (most recent call last):\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 623, in run_engine_core\n",
      "ERROR 08-08 13:52:17 [core.py:632]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 08-08 13:52:17 [core.py:632]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 441, in __init__\n",
      "ERROR 08-08 13:52:17 [core.py:632]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 86, in __init__\n",
      "ERROR 08-08 13:52:17 [core.py:632]     self._initialize_kv_caches(vllm_config)\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 158, in _initialize_kv_caches\n",
      "ERROR 08-08 13:52:17 [core.py:632]     self.model_executor.determine_available_memory())\n",
      "ERROR 08-08 13:52:17 [core.py:632]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "ERROR 08-08 13:52:17 [core.py:632]     output = self.collective_rpc(\"determine_available_memory\")\n",
      "ERROR 08-08 13:52:17 [core.py:632]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 237, in collective_rpc\n",
      "ERROR 08-08 13:52:17 [core.py:632]     result = get_response(w, dequeue_timeout)\n",
      "ERROR 08-08 13:52:17 [core.py:632]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 08-08 13:52:17 [core.py:632]   File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 224, in get_response\n",
      "ERROR 08-08 13:52:17 [core.py:632]     raise RuntimeError(\n",
      "ERROR 08-08 13:52:17 [core.py:632] RuntimeError: Worker failed with error 'Ninja build failed. Ninja output:\n",
      "ERROR 08-08 13:52:17 [core.py:632] ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "ERROR 08-08 13:52:17 [core.py:632] [1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "ERROR 08-08 13:52:17 [core.py:632] \u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "ERROR 08-08 13:52:17 [core.py:632] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "ERROR 08-08 13:52:17 [core.py:632] /scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "ERROR 08-08 13:52:17 [core.py:632] collect2: error: ld returned 1 exit status\n",
      "ERROR 08-08 13:52:17 [core.py:632] ninja: build stopped: subcommand failed.\n",
      "ERROR 08-08 13:52:17 [core.py:632] ', please check the stack trace above for the root cause\n",
      "ERROR 08-08 13:52:19 [multiproc_executor.py:140] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 636, in run_engine_core\n",
      "    raise e\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 623, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 441, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 86, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 158, in _initialize_kv_caches\n",
      "    self.model_executor.determine_available_memory())\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "    output = self.collective_rpc(\"determine_available_memory\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 237, in collective_rpc\n",
      "    result = get_response(w, dequeue_timeout)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py\", line 224, in get_response\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Worker failed with error 'Ninja build failed. Ninja output:\n",
      "ninja: Entering directory `/sailhome/benpry/.cache/flashinfer/80/cached_ops'\n",
      "[1/1] /scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "\u001b[31mFAILED: \u001b[0msampling/sampling.so \n",
      "/scr/benpry/conda/envs/vtc/bin/x86_64-conda-linux-gnu-c++ sampling/sampling.cuda.o sampling/renorm.cuda.o sampling/flashinfer_sampling_ops.cuda.o -shared -L/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -L/scr/benpry/conda/envs/vtc/lib64 -lcudart -o sampling/sampling.so\n",
      "/scr/benpry/conda/envs/vtc/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcudart: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "ninja: build stopped: subcommand failed.\n",
      "', please check the stack trace above for the root cause\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/entrypoints/llm.py:273\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m engine_args = EngineArgs(\n\u001b[32m    244\u001b[39m     model=model,\n\u001b[32m    245\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m     **kwargs,\n\u001b[32m    270\u001b[39m )\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/engine/llm_engine.py:497\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    495\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:126\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    125\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:103\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    100\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:77\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     74\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:514\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    513\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:408\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    405\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    407\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/utils.py:697\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scr/benpry/conda/envs/vtc/lib/python3.12/site-packages/vllm/v1/engine/utils.py:750\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    749\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    752\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    755\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=torch.bfloat16,\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=8192,\n",
    "    max_num_seqs=8,\n",
    "    max_logprobs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c5228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.generate(df.iloc[1][\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3001d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
