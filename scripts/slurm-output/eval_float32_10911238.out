]2;conda activate vtc]1;]2;cd ~/vlm-tg-context]1;]2;export LD_LIBRARY_PATH= ]1;]2;export CUDA_HOME="/usr/local/cuda-12.6" ]1;]2;export VLLM_USE_V1=0 ]1;]2;python scripts/call_lm.py --overwrite]1;INFO 08-08 12:24:46 [__init__.py:235] Automatically detected platform cuda.
data filepaths: ['/sailhome/benpry/vlm-tg-context/context_prep/wrong_across.csv', '/sailhome/benpry/vlm-tg-context/context_prep/shuffled.csv', '/sailhome/benpry/vlm-tg-context/context_prep/wrong_within.csv', '/sailhome/benpry/vlm-tg-context/context_prep/yoked.csv', '/sailhome/benpry/vlm-tg-context/context_prep/ablated.csv', '/sailhome/benpry/vlm-tg-context/context_prep/backward.csv']
Processing /sailhome/benpry/vlm-tg-context/context_prep/wrong_across.csv...
Applying chat templates...
Doing inference...
INFO 08-08 12:25:04 [config.py:3437] Upcasting torch.bfloat16 to torch.float32.
INFO 08-08 12:25:04 [config.py:1604] Using max model len 8192
INFO 08-08 12:25:05 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-VL-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=Qwen/Qwen2.5-VL-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":8,"local_cache_dir":null}, use_cached_outputs=False, 
WARNING 08-08 12:25:05 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
WARNING 08-08 12:25:05 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 08-08 12:25:06 [cuda.py:351] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.
INFO 08-08 12:25:06 [cuda.py:395] Using XFormers backend.
INFO 08-08 12:25:09 [__init__.py:235] Automatically detected platform cuda.
INFO 08-08 12:25:10 [__init__.py:235] Automatically detected platform cuda.
INFO 08-08 12:25:10 [__init__.py:235] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:10 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:11 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:11 [cuda.py:351] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:11 [cuda.py:395] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:12 [cuda.py:351] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:12 [cuda.py:395] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:12 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:13 [cuda.py:351] Cannot use FlashAttention-2 backend for dtype other than torch.float16 or torch.bfloat16.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:13 [cuda.py:395] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:14 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:14 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:14 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:14 [pynccl.py:70] vLLM is using nccl==2.26.2
INFO 08-08 12:25:14 [__init__.py:1375] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:14 [__init__.py:1375] Found nccl from library libnccl.so.2
INFO 08-08 12:25:14 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:14 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 08-08 12:25:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:16 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /afs/cs.stanford.edu/u/benpry/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 08-08 12:25:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_82fba530'), local_subscribe_addr='ipc:///tmp/benpry/e7fa7901-3498-4df0-b275-cc15b06e518d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:16 [parallel_state.py:1102] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 08-08 12:25:16 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:16 [parallel_state.py:1102] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:17 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 08-08 12:25:17 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-VL-32B-Instruct...
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:17 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-VL-32B-Instruct...
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:17 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-VL-32B-Instruct...
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:17 [model_runner.py:1083] Starting to load model Qwen/Qwen2.5-VL-32B-Instruct...
[1;36m(VllmWorkerProcess pid=1216472)[0;0m WARNING 08-08 12:25:17 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m WARNING 08-08 12:25:17 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m WARNING 08-08 12:25:17 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
WARNING 08-08 12:25:17 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
INFO 08-08 12:25:17 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:17 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:17 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:17 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-08 12:25:37 [default_loader.py:262] Loading weights took 19.38 seconds
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:38 [default_loader.py:262] Loading weights took 19.60 seconds
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:38 [default_loader.py:262] Loading weights took 20.07 seconds
INFO 08-08 12:25:38 [model_runner.py:1115] Model loading took 31.5839 GiB and 20.032080 seconds
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:38 [default_loader.py:262] Loading weights took 19.58 seconds
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:25:38 [model_runner.py:1115] Model loading took 31.5839 GiB and 20.735400 seconds
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:25:38 [model_runner.py:1115] Model loading took 31.5839 GiB and 20.963161 seconds
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:25:38 [model_runner.py:1115] Model loading took 31.5839 GiB and 21.104353 seconds
WARNING 08-08 12:25:39 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.
WARNING 08-08 12:25:39 [model_runner.py:1274] Computed max_num_seqs (min(8, 8192 // 32768)) to be less than 1. Setting it to the minimum value of 1.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m WARNING 08-08 12:25:40 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m WARNING 08-08 12:25:40 [model_runner.py:1274] Computed max_num_seqs (min(8, 8192 // 32768)) to be less than 1. Setting it to the minimum value of 1.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m WARNING 08-08 12:25:40 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m WARNING 08-08 12:25:40 [model_runner.py:1274] Computed max_num_seqs (min(8, 8192 // 32768)) to be less than 1. Setting it to the minimum value of 1.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m WARNING 08-08 12:25:40 [profiling.py:276] The sequence length (8192) is smaller than the pre-defined worst-case total number of multimodal tokens (32768). This may cause certain multi-modal inputs to fail during inference. To avoid this, you should increase `max_model_len` or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m WARNING 08-08 12:25:40 [model_runner.py:1274] Computed max_num_seqs (min(8, 8192 // 32768)) to be less than 1. Setting it to the minimum value of 1.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m WARNING 08-08 12:25:45 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 8192) is too short to hold the multi-modal embeddings in the worst case (32768 tokens in total, out of which {'image': 16384, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
WARNING 08-08 12:25:46 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 8192) is too short to hold the multi-modal embeddings in the worst case (32768 tokens in total, out of which {'image': 16384, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m WARNING 08-08 12:25:46 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 8192) is too short to hold the multi-modal embeddings in the worst case (32768 tokens in total, out of which {'image': 16384, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m WARNING 08-08 12:25:46 [profiling.py:237] The sequence length used for profiling (max_num_batched_tokens / max_num_seqs = 8192) is too short to hold the multi-modal embeddings in the worst case (32768 tokens in total, out of which {'image': 16384, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:30 [worker.py:295] Memory profiling takes 51.54 seconds
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:30 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:30 [worker.py:295] model weights take 31.58GiB; non_torch_memory takes 2.08GiB; PyTorch activation peak memory takes 6.81GiB; the rest of the memory reserved for KV Cache is 30.85GiB.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:30 [worker.py:295] Memory profiling takes 51.53 seconds
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:30 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:30 [worker.py:295] model weights take 31.58GiB; non_torch_memory takes 1.80GiB; PyTorch activation peak memory takes 6.81GiB; the rest of the memory reserved for KV Cache is 31.13GiB.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:30 [worker.py:295] Memory profiling takes 51.54 seconds
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:30 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:30 [worker.py:295] model weights take 31.58GiB; non_torch_memory takes 2.08GiB; PyTorch activation peak memory takes 6.81GiB; the rest of the memory reserved for KV Cache is 30.85GiB.
INFO 08-08 12:26:30 [worker.py:295] Memory profiling takes 51.54 seconds
INFO 08-08 12:26:30 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.90) = 71.33GiB
INFO 08-08 12:26:30 [worker.py:295] model weights take 31.58GiB; non_torch_memory takes 2.18GiB; PyTorch activation peak memory takes 6.81GiB; the rest of the memory reserved for KV Cache is 30.75GiB.
INFO 08-08 12:26:31 [executor_base.py:113] # cuda blocks: 15745, # CPU blocks: 2048
INFO 08-08 12:26:31 [executor_base.py:118] Maximum concurrency for 8192 tokens per request: 30.75x
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:33 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:33 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 08-08 12:26:33 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:33 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:36 [custom_all_reduce.py:196] Registering 516 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:36 [custom_all_reduce.py:196] Registering 516 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:36 [custom_all_reduce.py:196] Registering 516 cuda graph addresses
INFO 08-08 12:26:36 [custom_all_reduce.py:196] Registering 516 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1216476)[0;0m INFO 08-08 12:26:36 [model_runner.py:1537] Graph capturing finished in 4 secs, took 0.08 GiB
INFO 08-08 12:26:36 [model_runner.py:1537] Graph capturing finished in 4 secs, took 0.08 GiB
[1;36m(VllmWorkerProcess pid=1216471)[0;0m INFO 08-08 12:26:36 [model_runner.py:1537] Graph capturing finished in 4 secs, took 0.08 GiB
[1;36m(VllmWorkerProcess pid=1216472)[0;0m INFO 08-08 12:26:36 [model_runner.py:1537] Graph capturing finished in 4 secs, took 0.08 GiB
INFO 08-08 12:26:36 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 57.84 seconds
